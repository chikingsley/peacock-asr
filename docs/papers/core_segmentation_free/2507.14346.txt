Towards Accurate Phonetic Error Detection Through Phoneme Similarity
Modeling
Xuanru Zhou1, Jiachen Lianâˆ—2, Cheol Jun Cho2, Tejas Prabhune2, Shuhe Li1, William Li2, Rodrigo
Ortiz2, Zoe Ezzes3, Jet Vonk3, Brittany Morin3, Rian Bogley3, Lisa Wauters3, Zachary Miller3, Maria
Gorno-Tempini3, Gopala Anumanchipalli2
1Zhejiang University, China2UC Berkeley, United States
3UCSF, United States
âˆ—Project Lead, corresponding to: jiachenlian@berkeley.edu
Abstract
Phonetic error detection, a core subtask of automatic pro-
nunciation assessment, identifies pronunciation deviations at
the phoneme level. Speech variability from accents and dys-
fluencies challenges accurate phoneme recognition, with cur-
rent models failing to capture these discrepancies effectively.
We propose a verbatim phoneme recognition framework using
multi-task training with novel phoneme similarity modeling that
transcribes what speakers actually say rather than what theyâ€™re
supposed to say. We develop and open-source VCTK-accent , a
simulated dataset containing phonetic errors, and propose two
novel metrics for assessing pronunciation differences. Our work
establishes a new benchmark for phonetic error detection.
Index Terms : phonetic error detection, allophony, speech
recognition, phoneme similarity modeling
1. Introduction
Speech pronunciation assessment plays a crucial role in lan-
guage learning [1, 2] and diagnosis of speech disorders [3]. As
traditional human assessment is time-consuming and lacks uni-
fied standards, recent advancement has shifted to Computer-
Aided Pronunciation Training (CAPT) [4]. Parallel to the
bloom of textual large language models [5], recent speech re-
search focuses on audio-language foundation models [6, 7]
that unify multiple tasks including pronunciation assessment.
However, little evidence shows that multi-task learning im-
proves performance in specific tasks like pronunciation model-
ing. Thus, automatic pronunciation assessment remains a task-
specific area requiring domain-specific priors in model design.
Automatic pronunciation assessment involves detecting
multiple aspects of speech quality [4], including fluency, into-
nation, phonetic errors, prosodic features, stress patterns, and
rhythmic consistency. We focus on a core module: phonetic
error detection, which aims to identify pronunciation devia-
tions at a fine-grained phoneme level. This is fundamentally
anallophony problem [8]. Specifically, if a person pronounces
phoneme A, how confidently can we determine its relation to
phoneme Ë†A? For instance, can AI models reliably distinguish
between your pronunciation of the phoneme â€œTHâ€ when the
ground truth is â€œSâ€ in the word think , as illustrated in Fig-
ure 1? So far, we have found no evidence of such capabilities in
mainstream language learning platforms such as Duolingo [9],
Speak [10], or even GPT-4o V oice [11].
Earlier studies have treated pronunciation allophony as a
phoneme recognition or classification task, using normalized
classification logits scores as confidence measures [3, 12, 13,
14, 15]. These are also called phonetic replacement errors in
dysfluency modeling [13, 14, 16, 17, 18, 3, 15, 19, 20, 21, 22].
While [23] explored phoneme similarity in CTC training for
Speak Think 
  Correct:   TH  IH  NG  K 
You said:    S  IH  NG  K WPER 
AER 
Phoneme Similarity Error: TH     S 
4.2% 
10.7% 
83.3% 
âœ…
Correct âŒ
You said Articulator 
Visualization 
&
Feedback 
Relax the tongue 
Tip touches the teeth Figure 1: Demo of our method. Model transcribes user speech
into phoneme sequences, detects errors, scores using metrics,
and generates articulator visualizations and feedback [25]: re-
lax the tongue and place it against the roof of the mouth, with
the tip lightly touching the teeth.
dysarthric speech, their approach focuses on transcribing what
speakers should have said rather than what they actually said,
which is our research focus. Another relevant work [24] demon-
strated that self-supervised speech representations can implic-
itly leverage phoneme similarity in a way that aligns more
closely with human perception. However, it primarily focused
on analysis and still fails to transcribe what the person actually
said. Our work, on the other hand, focuses on the latter task,
which is distinct and more challenging .
In this paper, we propose a framework for verba-
tim phoneme recognition with phoneme similarity modeling,
trained using multi-task learning [26]: phoneme mapping and
connectionist temporal classification (CTC), aiming to accu-
rately transcribe the phonemes pronounced by the speaker.
Phoneme similarity modeling, which serves as a method to re-
veal allophony information, better aligns with both how hu-
mans produce speech and how the human ear perceives it. We
thus propose three novel phoneme similarity modeling meth-
ods: heuristic-based, articulatory-based, and Sylber-based [27]
methods. Through this modeling, we compute the similarity
score between each pair of phonemes. The process of integrat-
ing phoneme similarity into the training procedure is referred to
assoft-training . To providing training material that incorporates
phonetic errors and accurate labels, we follow the TTS-based
simulation approach [16] and generate a simulated dataset with
vowel and consonant substitutions, which we named VCTK-
accent and open-sourced. In addition, We introduce two novel
metrics: WPER andAER, which can be used not only for eval-
uating phoneme recognition models but also as pronunciationarXiv:2507.14346v1  [eess.AS]  18 Jul 2025assessment scores for speech.
Evaluated on the VCTK-accent test sets and real speech
datasets, our method shows impressive performance. Our train-
ing strategy- multi-task learning & soft-training significantly re-
duced the PER, WPER, and AER, proving its effectiveness.
This highlights the crucial role of phoneme similarity modeling
as a key approach to tackling the problem. Our work establishes
a new benchmark for phonetic error detection. The project page
is available at https://berkeley-speech-group.
github.io/Phonetic-Error-Detection/ .
2. Method
2.1. Data Simulation
To train the phoneme recognition model, accurate labels are es-
sential, mapping directly to the word being pronounced. Thus,
we follow the TTS-based dysfluency simulation pipeline de-
scribed in [16]. First, we inject phonetic substitutions into each
word from the text of the VCTK corpus [28] based on a prede-
fined set of common phoneme substitution pairs listed in Table.
1, which include vowel-to-vowel and consonant-to-consonant
substitutions respectively. Next, we input the modified IPA
sequences into the VITS [29] model to generate speech with
phonetic errors. These resulting (speech, modified phoneme se-
quence, reference word) pairs are used as training data for our
phoneme recognition model.
Table 1: Common CMU phoneme substitution pairs
Vowel Consonant
(AA, IY) (AE, UW) (AA, IH) (P, G) (T, ZH) (K, B) (M, S)
(OW, EH) (AO, EH) (UH, ER) (N, SH) (NG, F) (L, T) (R, D)
(AH, IY) (ER, OW) (AH, AE) (W, K) (TH, V) (DH, Z) (SH, HH)
2.2. Phoneme Similarity Modeling
Unlike [23] directly obtained the phoneme distance using
PanPhon tool [30], we propose three methods for modeling
phoneme similarity: heuristic-based, articulatory-based and
Sylber-based methods. By incorporating perspectives from
acoustics and syllables, these approaches more closely align
with human pronunciation and auditory perception. We obtain
a phoneme similarity matrix SâˆˆRNÃ—N, where each value in
the range (0,1), indicating the similarity between each pair of
phonemes. Nrepresents the size of the phoneme dictionary.
2.2.1. Heuristic-based
Heuristic-based method calculates similarity by comparing
phonemes based on eight features [31]: vowel or consonant,
vowel length, vowel height, vowel frontness, lip rounding, con-
sonant type, place of articulation, and consonant voicing. Each
feature is assigned a normalized weight, and the similarity
score between pairs of phonemes is computed by summing the
weights of matching feature values. In this study, the weights
are set as follows: 0.2, 0.1, 0.15, 0.15, 0.1, 0.2, 0.2, and 0.1, re-
spectively. The visualization of the phoneme similarity matrix
is presented in Figure 2.
2.2.2. Articulatory-based
We first construct a reference articulatory position for
each phoneme using the VCTK corpus and an acoustic-to-
articulatory inversion (AAI) model [25], employing a data-
driven approach. Next, we compute the L2 distance between
1.0
0.8
0.6
0.4
0.2
0.0
Figure 2: Heuristic-based phoneme similarity matrix
each pair of phonemes and apply min-max normalization to ob-
tain the similarity scores.
2.2.3. Sylber-based
Human speech segmentation is natually syllabic [27]. We then
utilize the Sylber [27] feature, as it offers a clean and robust
syllabic structure. First, we fine-tune Sylber using a phoneme
classification task with a single linear classifier layer, employing
the VCTK corpus. Then, following a similar approach outlined
in Sec. 2.2.2, we construct a reference feature for each phoneme
and compute the similarity score.
Overall, the heuristic-based method stems from the per-
spective of phoneme classification and definition. Both the
articulatory and Sylber-based methods are data-driven ap-
proaches: the former emphasizes the acoustic aspect, while the
latter focuses more on the syllabic aspect.
2.3. Verbatim Phoneme Recognition
We adopt the speech feature extracted from WavLM [32] to
train a verbatim phoneme recognition model with multi-task
learning and soft-training. The entire pipeline is illustrated in
Figure 3, and the model architecture and training objectives are
described in the following sections.
2.3.1. Model
The phoneme recognition model consists of a conformer [33]
encoder, an autoregressive projection layer, and a linear clas-
sifier. This conformer encoder consists of 8 layers with 4 at-
tention heads per layer. For the auto-regressive mechanism, For
the autoregressive mechanism, at each timestep, the model com-
bines the output features from the conformer encoder with the
embedding of the previously predicted phoneme to predict the
next phoneme, thereby effectively capturing the sequential de-
pendencies in the data.
2.3.2. Multi-task Learning and Soft-training
We employ weighted loss-based multi-task training pipeline
with two separate loss values: soft-CTC loss and soft-mapping
loss. For the phoneme similarity matrix, each column(row)VITS Conformer 
Projection 
Classifier 
Mapping Loss 
CTC Loss x8
Auto-Regressive 
Word:   Please  IPA:   p l eÉª[iË] z
CMU:   P L EY[IY] Z 
ðŸ—£
Phonetic Error 
Phoneme Similarity Matrix Predicted:          P  L  EY  Z
Ground Truth:    P  L  IY   Z
Heuristic        Articulatory        Sylber For the word Please ,                        
you are pronouncing it as P L EY Z , 
contains a phonetic error "IY" -> EY", 
the similarity between them is 79%.
Phoneme Alignment 
PLEYZ
Phoneme Label PP âˆ…LL âˆ…EY ... EYEY
EY
IYâ€¦50%
12% ... ...
EYIY
EY
IY
                  Training 
                           Inference Decode WavLM Feature 
 Correct                                      You said Tongue 
 
forward Articulatory 
feedback Prompt 
results Figure 3: Pipeline of phoneme recognition and error detection: phonetic error of "IY" -> "EY" with a similarity score 79% in the word
"Please", and the articulatory feedback is moving the tongue towards the front of the mouth.
represents the similarity score of this phoneme and all other
phonemes. We can treat this vector as a soft label of this
phoneme. For soft-CTC loss, we utilize the target phonemeâ€™s
soft label to weight the emission probability, thereby reduc-
ing the penalty for prediction errors between similar phonemes.
Traditional cross-entropy loss treats all phonemes as indepen-
dent and ignores their similarities. Thus, in soft-mapping loss,
we replace the one-hot encoded target with the target phonemeâ€™s
soft label. The complete loss is shown below:
L=Î»ctcÂ·LsCTC +Î»mapÂ·Lsmap
=âˆ’Î»ctcÂ·logï£«
ï£­X
zâˆˆB(y)TY
t=1NX
j=1Ë†S(zt, j)Â·pt(j)ï£¶
ï£¸
+Î»mapÂ·TX
t=1NX
j=1(pt(j)âˆ’Ë†S(yt, j))2(1)
Where Sdenotes the normalized phoneme similarity matrix,
B(y)denotes the set of all compatible alignments, yis the tar-
get phoneme sequence, pis modelâ€™s output emission probabil-
ity, and and Nis the phoneme dictionary size. In this work, we
setÎ»CTC = 0.8,Î»map= 0.2.
3. Experiment
3.1. Datasets
â€¢VCTK-Accent is a TTS-based [16] simulated datasets, ex-
tended from VCTK corpus [28], which contains vowel and
consonant phonetic errors, with simulation details provided
in Sec. 2.1. The total duration of the dataset is 323.9 hours.
â€¢L2-ARCTIC [34] includes recordings from 24 non-native
English speakers, each recording about one hour of read
speech from CMUâ€™s ARCTIC prompts. It provided forced-
aligned phonetic transcriptions, and annotated 150 utterances
per speaker for mispronunciation errors.
â€¢Speechocean762 [35] is an open-source non-native English
speech corpus, which consists of 5000 English sentences. All
the speakers are non-native, and their mother tongue is Man-
darin, and half of the speakers are children.â€¢MultiPA [36] was collected from real-world open-response
scenarios and consists of 50 audio clips, each lasting 10 to
20 seconds, from around 20 anonymous Mandarin-speaking
users practicing English with a dialog chatbot.
â€¢PPA Speech is collected from our clinical collaborators, and
consists of recordings from 38 participants diagnosed with
Primary Progressive Aphasia (PPA) [37]. We selected seg-
ments containing phonetic errors for evaluation.
For the above real speech datasets, we did the segmentation and
annotation ourselves, the process is: we first segment out the
words with phonetic error, then label the phoneme sequence that
the speaker actually pronounced as the target.
3.2. Training
The phoneme recognition model is trained with 90/10 train/test
split on VCTK-accent, with a batch size of 256. We use Adam
optimization and gradient clipping, and the learning rate is 3e-
4. The model is trained for 30 epochs with total of 75 hours on
an RTX A6000.
3.3. Evaluation Metrics
3.3.1. Phoneme Error Rate (PER)
PER measure of how many errors (inserted, deleted, and sub-
stitute phonemes) are predicting phoneme sequences compared
to the actual phoneme sequence. It calculated by dividing the
number of phoneme errors by the total number of phonemes.
3.3.2. Weight Phoneme Error Rate (WPER)
The single-word phoneme error rate (PER) is a relatively coarse
metric, as it only reflects the presence or absence of phonetic er-
rors, lacking the ability to capture nuanced pronunciation differ-
ences. To address this limitation, we introduce a more refined
metric: the Weighted Phoneme Error Rate (WPER). In the case
of substitutions, we replace the count of substitutions with the
sum of the phoneme similarities between the substituted pairs.
The equation is shown below:
WPER =D+(pr,ps)X
(1âˆ’S(pr, ps)) +I
L(2)